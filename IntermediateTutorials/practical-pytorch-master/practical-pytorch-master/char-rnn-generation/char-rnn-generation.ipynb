{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Generating Shakespeare with a Character-Level RNN\n",
    "\n",
    "[In the RNN classification tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) we used a RNN to classify text one character at a time. This time we'll generate text one character at a time.\n",
    "\n",
    "```\n",
    "> python generate.py -n 500\n",
    "\n",
    "PAOLTREDN:\n",
    "Let, yil exter shis owrach we so sain, fleas,\n",
    "Be wast the shall deas, puty sonse my sheete.\n",
    "\n",
    "BAUFIO:\n",
    "Sirh carrow out with the knonuot my comest sifard queences\n",
    "O all a man unterd.\n",
    "\n",
    "PROMENSJO:\n",
    "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
    "An as shalp will of that seal think.\n",
    "\n",
    "NUKINUS:\n",
    "And house it to thee word off hee:\n",
    "And thou charrota the son hange of that shall denthand\n",
    "For the say hor you are of I folles muth me?\n",
    "```\n",
    "\n",
    "This one might make you question the series title &mdash; \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more. See the [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) for more on that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
    "\n",
    "* http://pytorch.org/ For installation instructions\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) to get started with PyTorch in general\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general\n",
    "\n",
    "Also see these related tutorials from the series:\n",
    "\n",
    "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) uses an RNN for classification\n",
    "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) builds on this model to add a category as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('../data/shakespeare.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make inputs out of this big string of data, we will be splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-a.\n",
      "\n",
      "FLORIZEL:\n",
      "These your unusual weeds to each part of you\n",
      "Do give a life: no shepherdess, but Flora\n",
      "Peering in April's front. This your sheep-shearing\n",
      "Is as a meeting of the petty gods,\n",
      "And you the q\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=10):#初始值为1\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 39\n",
      " 40\n",
      " 41\n",
      "[torch.LongTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters *up to the last*, and the target will be all characters *from the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 9s (100 1%) 3.1250]\n",
      "Whowy we bmadl ods utw ta.sn the ge kelntre ginnu a sennd tin seaahirde leknd an te wtre tour thiteoth \n",
      "\n",
      "[0m 18s (200 2%) 2.6310]\n",
      "Whind th coe, wor mee he.\n",
      " _n goerd\n",
      "\n",
      "lSS, moutues thee?t han.\n",
      "ERWind sne an n'd the pine, loole me oei \n",
      "\n",
      "[0m 27s (300 3%) 2.4468]\n",
      "Whh[Iu youdeses or I lepe oo nre mefore bhang arg fouw ou annd acel thy bil of fit mis thos thee thand \n",
      "\n",
      "[0m 36s (400 4%) 2.3978]\n",
      "What mo the our thend she co int weand wenorconde deur nol wectoun sasto the ouk ang, bart him the ne  \n",
      "\n",
      "[0m 45s (500 5%) 2.4524]\n",
      "Wheses delil7 lonsang miofy foulndsthe hencour the and.\n",
      "Bund an ol setand and the be dist ance hist le \n",
      "\n",
      "[0m 54s (600 6%) 2.1673]\n",
      "Whais sher th lar.\n",
      "Hy bond wat my birt wouce anr abl has, were buxe preat sur and the weren so thand y \n",
      "\n",
      "[1m 3s (700 7%) 2.1075]\n",
      "Whg.\n",
      "\n",
      "IULAUACEGLO:\n",
      "Oe int shy and urf har, in cow the altid.\n",
      "\n",
      "MWok!\n",
      "\n",
      "Dall blarend iser but oue herthis \n",
      "\n",
      "[1m 12s (800 8%) 2.2183]\n",
      "Whad doger, sod derome an a met bad, Bor sever ound\n",
      "\n",
      "Por and coficen'd you the a of have with do sis d \n",
      "\n",
      "[1m 21s (900 9%) 2.4413]\n",
      "Whave the your to siod bupe hands stic:\n",
      "Sher the whe dand in tho hand shat; reasek's ler reightseg, se \n",
      "\n",
      "[1m 30s (1000 10%) 2.2703]\n",
      "WhTK I my the wist; you the und stis bes, cear, hese ill bist bes,\n",
      "And fave fore maint oad,\n",
      "Hot in im, \n",
      "\n",
      "[1m 39s (1100 11%) 2.2117]\n",
      "Whe be in, ace enow hes on ag so mey carure a not that corrinot as of reve let a hear be sveave liked: \n",
      "\n",
      "[1m 48s (1200 12%) 2.1321]\n",
      "When towred fom rean Ray thme grim svat may whit bye,\n",
      "Dy prins, mant wot of beeds thou the of not arto \n",
      "\n",
      "[1m 57s (1300 13%) 2.0625]\n",
      "Whe kind untdo sarme--\n",
      "How ther cour\n",
      "And willl that gor sindl; I or deap.\n",
      "\n",
      "ANLIAR:\n",
      "And cof chat\n",
      "Pill t \n",
      "\n",
      "[2m 6s (1400 14%) 2.1128]\n",
      "Whem, my wem whou tolfor his wir.\n",
      "\n",
      "Mhim fargo macle senterce, will be lone, yoy the wis is now at prea \n",
      "\n",
      "[2m 15s (1500 15%) 2.1713]\n",
      "Wheret.\n",
      "\n",
      "CELIO:\n",
      "To prisusud\n",
      "Thou, cold,\n",
      "In thee paths sich at ark mate, leath hean thy lorrow my I hav \n",
      "\n",
      "[2m 24s (1600 16%) 2.1023]\n",
      "Wh ghever but in leen we whe gone agreall feither hen his in say, Kerthfith that to the bear mo frees  \n",
      "\n",
      "[2m 33s (1700 17%) 1.9900]\n",
      "Whes om to beer cone the thine; for a ik hip is for the scill his douly furse ailf bay, som foll wors, \n",
      "\n",
      "[2m 42s (1800 18%) 1.8607]\n",
      "Whe poll you the chat couse the now nout thought I bringing fore hien, crowy,\n",
      "That be must farted ando \n",
      "\n",
      "[2m 51s (1900 19%) 2.1804]\n",
      "Where not he not he marce crime to here of I sentser be reprets, hake and most sime one stras to banmy \n",
      "\n",
      "[3m 0s (2000 20%) 2.0568]\n",
      "Whill in ano I-dow not'st thy so my wamed be that doud!\n",
      "\n",
      "PRINCA HOUCETEN ION:\n",
      "If chardinest are the is \n",
      "\n",
      "[3m 9s (2100 21%) 2.3046]\n",
      "Whilf bre'stall bodid\n",
      "\n",
      "TIUCEINTY:\n",
      "Wheel cithold not is and he the dane well he mwory'd have now pate b \n",
      "\n",
      "[3m 19s (2200 22%) 2.0989]\n",
      "Whave of fafled and in with gigsine if the speme,\n",
      "This soncine, of for grees all senterls a to reell e \n",
      "\n",
      "[3m 28s (2300 23%) 2.2013]\n",
      "Whe his grive allin the my you fording, whow fork and thinf and to wour conesen.\n",
      "\n",
      "ARIUS:\n",
      "Pat a pome a  \n",
      "\n",
      "[3m 37s (2400 24%) 1.6136]\n",
      "Whimge the tat that in I made sell flall thou of hath.\n",
      "\n",
      "OLOSTROCOLIO:\n",
      "I dow lode the has and hear not  \n",
      "\n",
      "[3m 46s (2500 25%) 1.9007]\n",
      "When worisel? reith lellouse Ratvere your thy of bretle are word,\n",
      "And wice halve; whis you, is pribe a \n",
      "\n",
      "[3m 55s (2600 26%) 1.9309]\n",
      "What lerst the whal sim as to my tell to have tho, are thy fasted acaut the have the cention unou this \n",
      "\n",
      "[4m 4s (2700 27%) 1.9375]\n",
      "Wh Your dives hever at the deager-t\n",
      "Thee a if not hove to my sort to stfrmear:\n",
      "Bo7dedtan\n",
      "That 'ther he \n",
      "\n",
      "[4m 14s (2800 28%) 1.9460]\n",
      "What thind eash this when of take the lord Tere no and thou some theresee, dour chim.\n",
      "\n",
      "KING CFizARIUST \n",
      "\n",
      "[4m 23s (2900 28%) 2.0156]\n",
      "What par-hut to coundy to maban a kerain to the mer conping five the shall, is my dame, ant swe knane  \n",
      "\n",
      "[4m 32s (3000 30%) 1.7096]\n",
      "When thou dake tranly in with to det to ment!\n",
      "\n",
      "GLUCES:\n",
      "I the hirs the may a his shan.\n",
      "\n",
      "BEDUTY, LICARDY \n",
      "\n",
      "[4m 41s (3100 31%) 1.9980]\n",
      "Wher.\n",
      "\n",
      "METRYA:\n",
      "Now sharsed:\n",
      "Why a comens or mayuse cist bey. Wond Cood.\n",
      "\n",
      "SPORK:\n",
      "In you losed the seon  \n",
      "\n",
      "[4m 51s (3200 32%) 1.9934]\n",
      "What the is nothercan\n",
      "Where that that such intap the sich to duck'y the seat theks of supher! Ame upes \n",
      "\n",
      "[5m 0s (3300 33%) 1.7898]\n",
      "Whet my such londers thou not bonder.\n",
      "\n",
      "pPRING ERICHARD:\n",
      "Whould by that king my thour it. my ild a hont \n",
      "\n",
      "[5m 9s (3400 34%) 2.0581]\n",
      "Whather the your I unleart, the beay our my goods with worss can and the come, and he is or what farri \n",
      "\n",
      "[5m 18s (3500 35%) 2.1026]\n",
      "When of the fall my lives? Tsurns you offer\n",
      "Weace thee wich then the leadids meave:\n",
      "\n",
      "HIANGARY HANGWIO: \n",
      "\n",
      "[5m 27s (3600 36%) 1.7695]\n",
      "Whe hourbelfair canting; ears am the trrust that he kid; my dome, and congrent you, chater's fase the  \n",
      "\n",
      "[5m 37s (3700 37%) 1.8205]\n",
      "Wher the taching if stort and seat then hold ah as for woul Cowed brown and citoder, sere a deave here \n",
      "\n",
      "[5m 46s (3800 38%) 1.9378]\n",
      "Where are you you mear gellour he thear but to derwead?\n",
      "\n",
      "FLOMVETHIUS:\n",
      "O, unst give the prom shall kids \n",
      "\n",
      "[5m 55s (3900 39%) 1.9012]\n",
      "Whe undrow hand; an and the sear trusies?\n",
      "Loan more he gray love me pearthaus I saigh briegtes in ere  \n",
      "\n",
      "[6m 4s (4000 40%) 1.8278]\n",
      "What brow out heard him bledsed a besal upon'd,\n",
      "And the love.\n",
      "\n",
      "KING GRED:\n",
      "But shold doulay loves are a \n",
      "\n",
      "[6m 13s (4100 41%) 1.5848]\n",
      "Whime of thou now the hors!\n",
      "\n",
      "BAMIOLANUS:\n",
      "For as loud stay his say,\n",
      "I werlt of his dear aply say, I day \n",
      "\n",
      "[6m 22s (4200 42%) 2.0523]\n",
      "Whiam the gove's stold no is moret you trear come then hem ling the take thus in the may not or tinger \n",
      "\n",
      "[6m 31s (4300 43%) 1.8354]\n",
      "Where to be lords\n",
      "Sold rut hand me haw word, for rour mont he saict hoble, me bortorn he sir, ungaring \n",
      "\n",
      "[6m 40s (4400 44%) 1.8031]\n",
      "Whess there suepery your my on there how ther upon in moy, sirst you are comaring fearthing, confull t \n",
      "\n",
      "[6m 50s (4500 45%) 1.9479]\n",
      "Whef thy braninot.\n",
      "So fathrart.\n",
      "\n",
      "CORDIUS:\n",
      "Whis she shall custiln you, resenat.\n",
      "\n",
      "\n",
      "ALAUSTER:\n",
      "Te not mus, \n",
      "\n",
      "[6m 59s (4600 46%) 1.8893]\n",
      "Whe deart all the see; nonk be come with thee thee am with all folling But as this in the sexternay\n",
      "th \n",
      "\n",
      "[7m 8s (4700 47%) 1.9067]\n",
      "Whild be sooll parge begnow is fath the gods instlour they which it dean her caurend have tries.\n",
      "\n",
      "Momb \n",
      "\n",
      "[7m 17s (4800 48%) 1.8587]\n",
      "Whelh made of the further bleantoy,\n",
      "And whemser, when am shorids to him of the there hen thou as so no \n",
      "\n",
      "[7m 26s (4900 49%) 1.8207]\n",
      "Whe stands of the not not with me your itian,\n",
      "And the gother-thee, whish my mare,\n",
      "Lord, Haftion, a cou \n",
      "\n",
      "[7m 35s (5000 50%) 2.0379]\n",
      "Where to he'l onat,\n",
      "Whock swo, carfurerer they and him'd her,\n",
      "I sund,\n",
      "Why, for and sich a brove me hea \n",
      "\n",
      "[7m 44s (5100 51%) 1.7203]\n",
      "Whirs condespind, ans be home meard one in this have arway in she himnong hirs hath the coul a partor  \n",
      "\n",
      "[7m 53s (5200 52%) 1.8444]\n",
      "Why Howise world.\n",
      "\n",
      "QUEENE:\n",
      "This me bathe from bly tis as my lave on is.\n",
      "Hange?\n",
      "\n",
      "KINIUS:\n",
      "Chath,\n",
      "Hearr a \n",
      "\n",
      "[8m 2s (5300 53%) 1.8464]\n",
      "Whith my come from you leak and therefore glose of enge sir; not by that.\n",
      "\n",
      "SDever sinters me to halk,  \n",
      "\n",
      "[8m 12s (5400 54%) 1.7026]\n",
      "WhO, I I may a cows your sir, thom it to stear a susers\n",
      "The sight come, not the to blost in your suck  \n",
      "\n",
      "[8m 21s (5500 55%) 1.6426]\n",
      "Whe brang not the yout sun a sto am and not Eden not pridise be that heve say!\n",
      "Let the from is may ven \n",
      "\n",
      "[8m 30s (5600 56%) 1.9242]\n",
      "Whercenter.\n",
      "\n",
      "DUKE ICHAND:\n",
      "Well the not bight unsend; and his face of you moonous twenter from your the \n",
      "\n",
      "[8m 39s (5700 56%) 1.7543]\n",
      "Wherest the tander\n",
      "The gook reshicy,\n",
      "Fread best that sid there the your'd gour tensted and I wind mut  \n",
      "\n",
      "[8m 48s (5800 57%) 1.7352]\n",
      "Whesself and ears of thing, the crobe\n",
      "Thell my love,\n",
      "Of I art sate wreaver, she earts hervers head lop \n",
      "\n",
      "[8m 57s (5900 59%) 1.8650]\n",
      "Whould what, and sting more to be sis. shorust me to our bigh!\n",
      "\n",
      "CORIIOL:\n",
      "My a not the so would arm I.\n",
      " \n",
      "\n",
      "[9m 6s (6000 60%) 1.6664]\n",
      "Whtranges, I wold hay sage your sir:\n",
      "And the give tas in one king be mither sonds dear le bear him my  \n",
      "\n",
      "[9m 15s (6100 61%) 1.7595]\n",
      "Wh RICHANRY:\n",
      "I lander him not restanter for where will his shall of them, what the king.\n",
      "\n",
      "ROMIO:\n",
      "I afm \n",
      "\n",
      "[9m 24s (6200 62%) 1.8230]\n",
      "Whe leam.\n",
      "\n",
      "DUKE VING OF MARINES:\n",
      "And we king nateryenf his be decountly fall did your fitht a mearms o \n",
      "\n",
      "[9m 34s (6300 63%) 1.9768]\n",
      "Whersooned\n",
      "Thein them be mis that with like delice.\n",
      "But abay bome and tore of came:\n",
      "Thou dowrice litsh \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9m 43s (6400 64%) 1.9031]\n",
      "Whem, when evering he seepin, and come,\n",
      "The trarked in the burslest in mearce of the go thou had detre \n",
      "\n",
      "[9m 52s (6500 65%) 1.8861]\n",
      "Whow cond hon him see,\n",
      "And sho?\n",
      "\n",
      "PEORTES:\n",
      "I lever and in then brift her ang there she hast her vither  \n",
      "\n",
      "[10m 1s (6600 66%) 1.7642]\n",
      "Whicled of ank of and the facising as a nisters\n",
      "To his hese it.\n",
      "\n",
      "AALIVA:\n",
      "And goinisht should shall won \n",
      "\n",
      "[10m 10s (6700 67%) 1.9259]\n",
      "Whome of her is the fort and up your parinisge to it of perepold\n",
      "his a known of of he pionsioble ears. \n",
      "\n",
      "[10m 19s (6800 68%) 1.7744]\n",
      "Whall not that love and lessat O Marden courbs.\n",
      "That wing Pillam; a doundry fiends a king of debtain's \n",
      "\n",
      "[10m 28s (6900 69%) 1.7149]\n",
      "Wher lest the conster of thou dedyear for of the good mistrief;\n",
      "Ctofe it one hatchards and it thee the \n",
      "\n",
      "[10m 38s (7000 70%) 1.9251]\n",
      "What is must amy do tha sirs, I have with whis Thousing this dreait.\n",
      "\n",
      "LARDIO:\n",
      "Who is be mersold.\n",
      "Iw'ti \n",
      "\n",
      "[10m 47s (7100 71%) 1.7493]\n",
      "Whe have you plore,\n",
      "And her soldack from o' the more anguer our do your his hands and your to amleman  \n",
      "\n",
      "[10m 56s (7200 72%) 1.5478]\n",
      "Wher that,\n",
      "An how you reconssomes says the wave good confore, her way,\n",
      "And are tis prounds a see and s \n",
      "\n",
      "[11m 5s (7300 73%) 1.8873]\n",
      "Whe down them letwas: from I as hone?\n",
      "\n",
      "ROMERO:\n",
      "That Bomelves\n",
      "And my to son come, not fair not side and \n",
      "\n",
      "[11m 15s (7400 74%) 1.6275]\n",
      "When\n",
      "Sonss meart as our not agaar\n",
      "Wither well a prate and mare, hensh father or dobe over dower\n",
      "Lill w \n",
      "\n",
      "[11m 24s (7500 75%) 1.7428]\n",
      "Whe:\n",
      "You addous.\n",
      "\n",
      "KING ERIANTA:\n",
      "\n",
      "KING ELIZARET:\n",
      "Which soward.\n",
      "\n",
      "CWALUSS OF YO:\n",
      "Not should he say, bove  \n",
      "\n",
      "[11m 34s (7600 76%) 1.6885]\n",
      "Whemcilefold thee solest there her to to the reis at by of me to sake my kivernoved of henself;\n",
      "To der \n",
      "\n",
      "[11m 43s (7700 77%) 1.5164]\n",
      "Whis trotentred plate.\n",
      "\n",
      "POLXENENRUCHIO:\n",
      "The one a well hear let the dauses but in allo such dush: and  \n",
      "\n",
      "[11m 52s (7800 78%) 1.8695]\n",
      "What to twell to re death so her it.\n",
      "LE whrod sirn may.\n",
      "\n",
      "SALUNIO:\n",
      "What sopest that come your, day as g \n",
      "\n",
      "[12m 2s (7900 79%) 1.8702]\n",
      "Wher the seed of yet mades her lord,\n",
      "And the have the way be spepinged the in thees is of my him would \n",
      "\n",
      "[12m 11s (8000 80%) 1.9772]\n",
      "Whish I am prew tity.\n",
      "\n",
      "MENENIUS:\n",
      "God to grace,\n",
      "Thought's the wast of a his lings\n",
      "Thought than yourself \n",
      "\n",
      "[12m 20s (8100 81%) 1.6622]\n",
      "What be,\n",
      "To them yout to unding be worght to be man of denother dun but yet her toper for the is any h \n",
      "\n",
      "[12m 30s (8200 82%) 2.0747]\n",
      "When me dies? for loves, he his dedepy!\n",
      "\n",
      "CLARINGAS:\n",
      "Of great, like to see a coydy deas ere so?\n",
      "Being h \n",
      "\n",
      "[12m 39s (8300 83%) 1.8400]\n",
      "Whrow, this in words say do no bloor my tay. What shall to then the plare the vile, I lick a hone memi \n",
      "\n",
      "[12m 48s (8400 84%) 1.8789]\n",
      "Whom no nenthater to the woldiery,\n",
      "That in is itseard\n",
      "Cain as do pread\n",
      "of her rotle so eny to sook pai \n",
      "\n",
      "[12m 57s (8500 85%) 1.1704]\n",
      "What son, sir me ondas a liel, where he fit the valtaces. Slaid sweit thee,\n",
      "This like it proth saige;  \n",
      "\n",
      "[13m 7s (8600 86%) 1.6584]\n",
      "What desis him to me angrother mate; us you prantly norruth you gaid that soul your give lone the fort \n",
      "\n",
      "[13m 16s (8700 87%) 1.7219]\n",
      "Whes some then set\n",
      "Of the!\n",
      "\n",
      "MENGBROLANUS:\n",
      "Why will me villable,\n",
      "And so inent preave or mans the littre \n",
      "\n",
      "[13m 26s (8800 88%) 1.7155]\n",
      "While of me the care to feeca\n",
      "gothing eek-shands of my pring all pleaw sich all grame: we adous haste  \n",
      "\n",
      "[13m 35s (8900 89%) 1.9020]\n",
      "Whes: I servies?\n",
      "I'll subbed thy sages mind at the way stand come a kinds, you not more brother should \n",
      "\n",
      "[13m 44s (9000 90%) 1.6767]\n",
      "Whese comes of you milixt sholoops, no tha frince, hobe\n",
      "Und with so hope slay it speet it, and thou bu \n",
      "\n",
      "[13m 54s (9100 91%) 1.6771]\n",
      "Whirl'd me peasul near, of from a deen, and with the fier,\n",
      "In the lord:\n",
      "I letter\n",
      "Were, reture\n",
      "speaute, \n",
      "\n",
      "[14m 3s (9200 92%) 1.8637]\n",
      "Whow Leive, she bessiancain of thee confark that honester rajent: here but hear were it to this the wa \n",
      "\n",
      "[14m 12s (9300 93%) 1.7813]\n",
      "Whes or swee, see, Boot the prook, I do chell to the haze sornies his fare\n",
      "I warforing the grace\n",
      "Rour  \n",
      "\n",
      "[14m 22s (9400 94%) 1.8024]\n",
      "Whe tell, with perester.\n",
      "\n",
      "GLOUCESTER:\n",
      "Now, I made\n",
      "To there lay and heart.\n",
      "A canst of Clitted\n",
      "Neseed\n",
      "Be \n",
      "\n",
      "[14m 31s (9500 95%) 1.7161]\n",
      "When:\n",
      "Whith for I bight to never be us toon pleather.\n",
      "\n",
      "TRANIO:\n",
      "Shirse the plewa,\n",
      "Shall do yout than fi \n",
      "\n",
      "[14m 40s (9600 96%) 1.3934]\n",
      "Whey no but were bethoul, I warmits to paskens:\n",
      "Whatn,\n",
      "Come were these but you love of my comple he wo \n",
      "\n",
      "[14m 49s (9700 97%) 1.5234]\n",
      "Which come make me, sick\n",
      "For. First like of make the call'd in my death to the loves as I made, the wa \n",
      "\n",
      "[14m 59s (9800 98%) 1.8907]\n",
      "Whme the come,\n",
      "What with lainst of he soon not the reatcorus nexide much it is make and lord and whene \n",
      "\n",
      "[15m 8s (9900 99%) 1.4264]\n",
      "Which shall I make epterny do be if your stried as me to you and for the ewhould hemsed of as met.\n",
      "\n",
      "DO \n",
      "\n",
      "[15m 17s (10000 100%) 1.7828]\n",
      "What the hath and against the codved with mids;\n",
      "it nof and give will in which a lest horful but world  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#初始迭代次数：2000\n",
    "n_epochs =10000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.0005\n",
    "#初始学习率：0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Training Losses\n",
    "\n",
    "Plotting the historical loss from all_losses shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x9ece9b0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYk9XZx/HvnVkY9nVYBGTYBAEBZUQQUYqIa7V117rW\npW9ri7ba1qXSFquitmqrrbvWtpa61aqIoigoooKDbLIPguww7Aww+3n/yJNMkkkymWFgSPh9riuX\nT56cJOeZ4J2Tc+5zjjnnEBGR1OKr7wqIiEjdU3AXEUlBCu4iIilIwV1EJAUpuIuIpCAFdxGRFKTg\nLiKSghTcRURSkIK7iEgKSq+vN27Tpo3Lycmpr7cXEUlKs2fP3uKcy66uXL0F95ycHPLy8urr7UVE\nkpKZfZtIOXXLiIikIAV3EZEUpOAuIpKCFNxFRFKQgruISApScBcRSUEK7iIiKSjh4G5maWY2x8wm\nRnnsGjMrMLO53u36uq1mpaUbd/On95eypbD4QL2FiEjSq0nL/WZgcZzHX3bODfRuz+5nvWJaUVDI\nYx/ls7Ww5EC9hYhI0ksouJtZJ+Bs4IAF7USl+QyAsoqKeq6JiMihK9GW+6PAr4B4EfUCM5tvZq+Z\nWedoBczsRjPLM7O8goKCmtYVgHQvuJdXuFo9X0TkcFBtcDezc4DNzrnZcYq9DeQ45/oDU4AXoxVy\nzj3tnMt1zuVmZ1e77k1UlS13BXcRkVgSabkPA841s1XAf4CRZvav0ALOua3OucAI5zPAoDqtZYh0\nn7/KarmLiMRWbXB3zt3hnOvknMsBLgU+cs5dEVrGzDqE3D2X+AOv+yXYci9XcBcRiaXWS/6a2Tgg\nzzn3FjDGzM4FyoBtwDV1U72q0tPU5y4iUp0aBXfn3DRgmnc8NuT8HcAddVmxWJQtIyJSvaSboaps\nGRGR6iVdcFe2jIhI9ZIuuCtbRkSkekkX3NVyFxGpXtIF98o+dw2oiojEknTBXXnuIiLVS7rgrjx3\nEZHqJV1wV5+7iEj1ki64K1tGRKR6SRfc1XIXEale0gV3ZcuIiFQv6YK7Wu4iItVLuuAebLkrFVJE\nJKakC+6BlnupWu4iIjElXXA3M9J8pj53EZE4ki64g7/1rj53EZHYkjK4Z/hMfe4iInEkZXBXy11E\nJL6kDO7paT7NUBURiSMpg7ta7iIi8SVlcE9XtoyISFxJGdzVchcRiS8pg7u/5a7gLiISS1IGd7Xc\nRUTiSzi4m1mamc0xs4lRHmtgZi+bWb6ZzTSznLqsZKR0n0957iIicdSk5X4zsDjGY9cB251zPYBH\ngAf2t2LxqOUuIhJfQsHdzDoBZwPPxihyHvCid/wacKqZ2f5XL7r0NGXLiIjEk2jL/VHgV0CsiNoR\nWAPgnCsDdgKt97t2MajlLiISX7XB3czOATY752bHKxblXJXoa2Y3mlmemeUVFBTUoJrhlC0jIhJf\nIi33YcC5ZrYK+A8w0sz+FVFmLdAZwMzSgebAtsgXcs497ZzLdc7lZmdn17rSarmLiMRXbXB3zt3h\nnOvknMsBLgU+cs5dEVHsLeBq7/hCr8wBi77pPh9l5epzFxGJJb22TzSzcUCec+4t4Dngn2aWj7/F\nfmkd1S8qn89QJqSISGw1Cu7OuWnANO94bMj5IuCiuqxYPGkGFeqWERGJKWlnqGpAVUQktqQM7j4z\nKg5cl76ISNJLyuCulruISHxJGdz9A6oK7iIisSRlcE8z04CqiEgcyRnc1XIXEYkrKYO7zwytGyYi\nEltSBvc0HxpQFRGJI0mDu7plRETiScrg7tOAqohIXEkZ3NVyFxGJLymDu880iUlEJJ6kDO5pPnXL\niIjEk5TB3WeoW0ZEJI7kDO4+Qw13EZHYkjK4a/kBEZH4kjO4K1tGRCSupAzuPjOcgwO4TauISFJL\nyuCe5jNASxCIiMSS3MFdLXcRkaiSMrj7zB/ctTKkiEh0SRnc07xaq+UuIhJdUgb3QMtdfe4iItEl\nZXAP9Lkr111EJLqkDO7pXnAvU3AXEYmq2uBuZllmNsvM5pnZQjP7fZQy15hZgZnN9W7XH5jq+qV7\nne7qlhERiS49gTLFwEjnXKGZZQCfmtm7zrkvIsq97Jz7ad1XsapAt0xpudJlRESiqTa4O/800ELv\nboZ3q9cmc0aaumVEROJJqM/dzNLMbC6wGfjAOTczSrELzGy+mb1mZp1jvM6NZpZnZnkFBQW1rnS6\nz1/tMrXcRUSiSii4O+fKnXMDgU7AYDPrF1HkbSDHOdcfmAK8GON1nnbO5TrncrOzs2tdaQ2oiojE\nV6NsGefcDmAacEbE+a3OuWLv7jPAoDqpXQyBAdWycgV3EZFoEsmWyTazFt5xQ2AUsCSiTIeQu+cC\ni+uykpHSvT73Uq0/ICISVSLZMh2AF80sDf+XwSvOuYlmNg7Ic869BYwxs3OBMmAbcM2BqjBAhk+p\nkCIi8SSSLTMfODbK+bEhx3cAd9Rt1WJTKqSISHxJOUM1mAqpPncRkaiSMrhrhqqISHzJGdzVLSMi\nEldyBnfNUBURiSs5g7uXLaOWu4hIdEka3LVZh4hIPMkZ3JUtIyISV1IG98x0f7WL1S0jIhJVUgb3\nBulpABSXltdzTUREDk1JGtz91S5Ry11EJKqkDO6Z3iSm4lIFdxGRaJIyuPt8RkaaqeUuIhJDUgZ3\n8Pe7q+UuIhJd0gb3zHQfJeUaUBURiSZpg3uDdJ9a7iIiMSRtcM9M91FcpuAuIhJN0gb3Buk+isvU\nLSMiEk3SBvdmWRns3Fda39UQETkkJW1wb988i407i+q7GiIih6SkDe4dmmexYWcRzmnxMBGRSEkb\n3Ns1y6K4rIIde9U1IyISKWmDe4fmDQHYuEtdMyIikZI2uLdv3gBQcBcRiSZpg3uTBhkA7Ckuq+ea\niIgceqoN7maWZWazzGyemS00s99HKdPAzF42s3wzm2lmOQeisqEaZvjXdC/SLFURkSoSabkXAyOd\ncwOAgcAZZjYkosx1wHbnXA/gEeCBuq1mVVmZ/qrv04YdIiJVVBvcnV+hdzfDu0XmH54HvOgdvwac\namZWZ7WMIitDuzGJiMSSUJ+7maWZ2VxgM/CBc25mRJGOwBoA51wZsBNoXZcVjRToltlXouAuIhIp\noeDunCt3zg0EOgGDzaxfRJForfQqs4vM7EYzyzOzvIKCgprXNkRGmo90n1Gk9WVERKqoUbaMc24H\nMA04I+KhtUBnADNLB5oD26I8/2nnXK5zLjc7O7tWFQ6VlZHGvhINqIqIREokWybbzFp4xw2BUcCS\niGJvAVd7xxcCH7mDsC5Aw8w0pUKKiESRnkCZDsCLZpaG/8vgFefcRDMbB+Q5594CngP+aWb5+Fvs\nlx6wGofo0qoRK7fsORhvJSKSVKoN7s65+cCxUc6PDTkuAi6q26pVr2e7pkxeuPFgv62IyCEvaWeo\ngn9lyG17SijRjkwiImGSOri3bepfX6agsLieayIicmhJ7uDezB/cN2vxMBGRMMkd3JtmAbBpl1ru\nIiKhkjy4e90yu9VyFxEJldTBvXWTBmSkGYs27K7vqoiIHFKSOrin+YyzjunAO/PXU1qujBkRkYCk\nDu4Aw3q0YVdRGTf+I0+bZYuIeJI+uDfO9M/Dmrq0gF37tBSBiAikQHDPyqi8hNIKdc2IiEBKBPe0\n4LFmqoqI+KVAcK+8BAV3ERG/pA/uDdJDWu7KmBERAVIguIe23Oeu2VGPNREROXQkfXAPbbn/6rX5\n9VgTEZFDR9IHdxERqSrpg/sRLRrSoXlWfVdDROSQkvTBPc1n/PGiAcH7yzdpnRkRkaQP7gDFZeXB\n41dnr63HmoiIHBpSIrgP69EmeLxs025+++bXlFdonRkROXxVu0F2MgjNmJm2tACAy044kt7tm9VX\nlURE6lVKtNxFRCRcygT3H53SLez+3pLyGCVFRFJfygT3O848mv87pXvw/p5iLf8rIoevlAnuAE2z\nKocQFNxF5HBWbXA3s85mNtXMFpvZQjO7OUqZEWa208zmerexB6a68YUG99Xb9tZHFUREDgmJtNzL\ngFudc0cDQ4CbzKxPlHLTnXMDvdu4Oq1lgpo0qAzunyzbUh9VEBE5JFQb3J1zG5xzX3nHu4HFQMcD\nXbHa6Nm2afB4w8599VgTEZH6VaM+dzPLAY4FZkZ5eKiZzTOzd82sb4zn32hmeWaWV1BQUOPKVqdf\nx2bcc15fRh3djhUFe/j5y3MpVN+7iByGEg7uZtYEeB24xTm3K+Lhr4AuzrkBwGPA/6K9hnPuaedc\nrnMuNzs7u7Z1jldHrhyaw7kDjwDgjTnreC1vTZ2/j4jIoS6h4G5mGfgD+0vOuf9GPu6c2+WcK/SO\nJwEZZtYmstzBctrR7YLHaT6rr2qIiNSbRLJlDHgOWOycezhGmfZeOcxssPe6W+uyojXRMLNyOYJ1\nO4pYunE3L838tr6qIyJy0CWytsww4EpggZnN9c7dCRwJ4Jx7ErgQ+LGZlQH7gEudc4fEyl1PfryC\nf33xLYXFZVw0qDOZ6SmV2i8iElW1wd059ykQt2/DOfc48HhdVaouXH7Ckfx75mqA4KDqpl1FdG7V\nqD6rJSJyUKRsM/ae8/pVObd+h9IjReTwkLLBPdpA6p1vLKiHmoiIHHwpG9wBzuzXPuz+ioI9vDl3\nHcs27Wbi/PX1VCsRkQMvJTbriKW0vOqY7s3/mRs8Pqf/EQezOiIiB01Kt9zLKyrquwoiIvUipYN7\nmbeP6rNX5UZ9fPmm3QezOiIiB01KB/e7z+nD0G6tObFHay4c1KnK46c98kk91EpE5MBL6eB+VLum\nTLhxCI0y0xndx78kwXcHhPezb9tTwucrtrJ2u9Z/F5HUkdLBPdTovu1Zfu+ZDM5pGXb+z1OWcdkz\nX3C6WvEikkIOm+AOkJHmIyMt/JK37ikBYE9JOd9u3VMf1RIRqXOHVXAHGHl027D7E+dvCB6f8tA0\n1u3YxyGyLI6ISK0ddsG9bdMsfj7qqJiPDxv/EQ9OXsrWwmIqKhxvzFlLablSKkUkuaT0JKZYbjy5\nmz8H3oy/fLgcgCNbNQpuqv3EtBU8MW0Fvds3ZcnG3WwtLOH64d2Cz89btY2yCseQbq3rpf4iItU5\n7Fru4F/v/Reje3Fs5xbBcy0aZVQpt2SjPw9+x95SgGB3zYVPfs6lT39xEGoqIlI7h2VwDxjRK5vx\n5x/Dgt+NpkGcdd5nf7udeWt20PWOSTw6ZdlBrKGISO1YfQ0e5ubmury8vHp572hWb93L8zNW0rlV\nI96Zv56vVu+o9jlPXzmI0X0rFydbs20vO/aWckyn5geyqiJyGDOz2c656NPuQ8spuEfX/c5JlFfE\n/9tkpvtY9oczg/dzbn8HgFXjzz6gdRORw1eiwf2w7paJp2GGfx/Wa4flxCxTUlbB3pKyKuc/WLTp\nQFVLRCQhCu4xZGX4/zTXD+9G68aZMcv1GTuZHzz7BTO/qdwP/IZ/5PHzl+fGfI6IyIGm4B7DLV4u\nfKtGmcy4fWTcsjPyt3JJRPbMG3PWUVKm/HgRqR8K7jFcMaQLq8afTcPMNLK8LppQbZrEbs0HHPWb\ndznrz9PZWljMx8sKqAjpwx/5p2ncP2lxndZZRCRAwT1Bb/zkRCbcMCR4v8L5lxSuzqINuxj0hylc\n/fws3l+0iS2FxRQWl/FNwR6e+uSbA1llETmMKVumhtbv2MfVz8/i9jN7M7J3W7reManGr3FE8yzW\n7ywCYEi3VnTLbsKJ3VuT26UV7ZtnUV7huO3VeVxzYg4DQiZaxbNjbwmbdhXTq33TGtdHRJKHUiEP\nkunLC1i6cTd5q7bz3sKN+/VaPdo2YcovTmFFQSGn/uljurRuxMe//A479pbQpEE66Wmxf2id/OBU\nVm/bqzRMkRRXZ6mQZtbZzKaa2WIzW2hmN0cpY2b2FzPLN7P5ZnZcbSuebIb3zOb64d148spBnOtt\nBPKjU7pV86zo1m3fR8HuYs58dDrgT8dct2MfA8d9wPH3TqGwuGraZUBgXRwREUisz70MuNU5dzQw\nBLjJzCI7m88Eenq3G4En6rSWSeLi3M4AXDU0pzKV8qSuCT/fZ3D8vVMo8VahXLJxN8PGfwTA9r2l\n/Pq1+QB8vmIr05ZuJuf2d/j3zNVhrxGYeLVpVxFfr9u5fxckIkmrxt0yZvYm8Lhz7oOQc08B05xz\nE7z7S4ERzrkNMV4mZbplYlm7fS/rdxQxsHMLCgqLg0F6f3TLbsw3BfE3FFk87gwaZqbRZ+x77C0p\nZ/7vRtMkMx2fz/b7/UWk/h2QGapmlgMcC8yMeKgjsCbk/lrv3GGrU8tGDO7aisx0Hx1bNGTG7SN5\n8IL+YWX+fOlApt02Iurz+0dZn6a6wA6wr7ScvSVl7C0p97/O797nrv8t4OQHp/LGnLU1v5AIM/K3\nkHP7O2zzdrCqzjcFhaxRl5HIQZdwcDezJsDrwC3OuV2RD0d5SpWfBGZ2o5nlmVleQUFBzWqa5Dq2\naMjFx3fm0UsGAnDFkCM5b2BHcto0jlr+Ryd3r9X7PDR5KX3GTg47N2HWGlZv28uz01cCsK+knM27\nisjfvLvGr//MdH/65pzV28POX/b0F/x5yvIq5Uf+6WOGPzi1xu8jIvsnoeBuZhn4A/tLzrn/Rimy\nFugccr8TsD6ykHPuaedcrnMuNzs7uzb1TXpnHtOeG0/uxi9H9w6e+643EBsqM84SxAA/G9kj6vkJ\ns1ZHPQ/+3Pxnp3/D0WPfY/B9HzLq4U/4pqCQ5z5dyVcRwXrDzn1s9NI1wb+WfUWFI9PL2AmscR/w\n+TdbeUTLIYscMhLJljHgOWCxc+7hGMXeAq7ysmaGADvj9bcfzhqkp3HnWUfTPGRzkN+cfXTweMHv\nRjP9V9/h2CPD89sHdWkZPH744gE0y6q6uUh1nHP84Z3wWbG/e3sR90xcxPl/+4wFa3eycssenHMM\nvf8jhtz/IeAfpO16xyS63TmJ9DT/j7RbX50X9T3u/t/XMd//sxVb2FVUGvNxEak7iWyzNwy4Elhg\nZoHVsO4EjgRwzj0JTALOAvKBvcC1dV/V1NWuWRZHd2jG+h37aJqVQVMvcK8afzZbCospLqvgiOZZ\nvPf1RoZ2b02LRpls31PCtGWbmZHvX7DsqqFd+Mfn38Z9n2hj558sq+we++7jnwJwz3l9g+cKi8tY\nvKGyF25LYWVf+7od+xj39kIevnhg8Nw/v/iWe77Xz3u/yjfcsbeEy5+ZyYhe2Yzs3ZZPlm3hzH7t\nGdm7LS29hdkC5f3tCRHZH9UGd+fcp0TvUw8t44Cb6qpSh6NJY06KGnzbNGkQPD7zmA7B45aNM3np\n+iHsKymn3DmmLtlcbXAvKitPqC53v7kwePzRks0s3VgZ3Get3BY8DmQAvT0vvAfurjcWcHb/Dhzb\nufLXxq59/hz9ZRt3M22p/wtlyuJNjOiVzd+vHcxn+Vu4/NmZ5HZpyWs/PjHs9Soq3AHJ9vnzlOU0\na5jOtcMST1ctKavg6/U7Oe7IltUXFqlHWlvmEGFmtQpgDTPTaNIgnXP6d+DV/xvK+cf5k5S6ZTeu\nMlv12601z1oZM2EOf526gqM7NItZ5q2I4P7SzNVc/sxMZuRvCZ57YPISAErKw7/BCnb719q5/Fl/\nAlbet+F9/z95aTbd7pzElDpcI3/z7iJufWUej0xZxu/fXlSj5943aTHn/+0z8jcX1ll9RA4EBfcU\nYWYcn9OK8ef358JBnXjhmuMBGBNj4LWmrhhyJE9eMSjqY5+t2Br1/PX/qJzH8M58/xDMlsLisDLL\nNxfS77fh2T0BzjkmLdhY5bVC7Ssp5/GPlnPbq/MoK6/gnMem0/OuSWzeVRS1PMD4d5fw+leJp4VW\nVLhgl9HC9f6JYdv3JpYKKlJfFNxTTGa6jz9eNIAurf0plr8Y3YtJY4Zz9dAuUcvHapE/f03lHInr\nT+rK5YOP5PS+7RKqQ9OsRIZy/KKteb9h5z4+X7E1mKsf8O3WPXxTUMiohz9m/lr/Hrf3vLOIP76/\njNdmr+Xzb7by9bpdlJY7Bt/3YfB52/eUsK+knM9WbOGSpz6nsCj2Mg6RisvK6XbnJB710jzN66EM\n7UIrK6+gvtZoAv8XXE3e3znHt1urnzMhyS3x/wslafU5ohm3nt6LF0P65D/55XcorahgRv4Wxr65\nkF+e3ouubRrzk5e+AmBk73aM7tOOo9o15bbTewWfN+UXJ7OrqIzz//ZZzPcb2LkF05dvifl4dYbe\nH3027ykPTQsen/v4DFaNP5slIYO9t75SNYNnS2ExuX+YQr+Ozfh6nb9sn4gvtMi9b3fuK2XA79/n\n0uM78+sz/Cmrf/5wOfPWVm6aHgimm3cXMfjeDxl7Th/KKio4d0BH2jfPYk9xGQ3SfXEXe0vUzr2l\nfLlqG6P6VP1yXbt9Lyc9MJV7vtePK4dE/wKP9MTHK3jwvaW8/dOT6NexmQaw69DekjLW7yiiR9sm\n9V0VBffDRbOsDP530zC27SlmyuLNHNm6EQDds5tw3oCOwdTMH4/oTr8j/LNjn76q6gznHm2jLyl8\n8lHZ/Ojkbsxfu5Nrh+XQ++736qzubZs2YPPu4irn/fn5lQE3ssw78zdw07/9X1aBwA7+NfZj+XT5\nFq54zt///58v1zDm1J7Bx6YtLWBw11YAlHlr+Kzdvg+AcRP9ffd5q7bz9FW59P3tZEYd3ZZnrz4+\n+PyLn/qczDQfL/5wMEPu/5DbRh/FJccfCfiziQBaNKq6CcxPJ3zF9OVbmHXXqbRtmhX2WKDv//2F\nGxMO7i994Z8L8d3HP2Vk77YM6NSCm0f1rOZZ1ftoySZu/Mds5ow9LZjxdbi54R95zMjfysr7z6r3\nL011yxxGBnZuwcje7bjv+8eEnQ/Nuf/1Gb05u3+HyKdWMf788Ncor6hgWI82/HhEd7Iy0mjaoO7a\nDadFabEC3DMx/mBoILDXxKSvw6dnXPPCrLD7gWyhotJyhtz3YZVfMO8v2hTcIH3K4s1Vnvtp/ha2\n7SmhYHcxv359QfCxgeM+YOC4D4gmEMCLSsK7sN5dsIFrXvgSIDi5LJ5vCgr569R8CkLGPT5asnm/\nJp+9Nnstr+b5Vx55dMpyyiocKxJYJqMuPPfpShatj/1FXR8Cqcml5fXXTReg4C61cnFuZ35/bmU+\nfFnEP+a9pZX95d8/1p/Bc8FxneK+ZrfsxlxzYk7YudP7tuN35/bluhqsrllbyzbtrrLK5rJN0bNi\nrnsxj40xBm1viDL4+9rsygHcT/PDl96I3G6xqLSc2V7W0G2vzmODN1P4nMemh5W7+T+Vm7AHZjT/\n/OW53PXGAqK59u9f8tDkpVHHOeL12U9dsjls8tnnK7Zy83/m4Jx/U5lfequVJvJatTX+3SXBL5HA\ne9wzcVGVv0ltFZeV1+kaSGUV9b9/soK71IrPZ1xyfOWKE+UV4f9D9w7ZEWrceX25ckgXxp3XlwW/\nG83K+88C4BzvF0IzbwC2qKScE7xuD4CfjzqKxy47jow0X9gs3gNl9COf1PlrrtuxLxgEA37+cuXx\nJ8sKwrZb3LSriN53v8cFT3zG1+t2hn0p7CoqY9zbiygrr2BXUWlwaWjwB/cde0t4Y846Xpq5mtLy\nCq59YRYXP/U54N9BLF4qbLEX8Kd6S0kHBlx3FZVy7d+/5OrnK3/BXPHcTN6cu559IV/gP3lpdnAy\nTFFpRTArqrC4jPzNu1m0fhf/mbWa216dR++732XCrNXBL4GSsgqKq5mD8eTHK8K+RIpK/fWtqKPv\nkVtfmcfwB6dWW49ElZbVf8tdfe5Saw1C1r8pi/i/7IVrj2f2qu0c2boRTbMygrNWAwKDl/edX8q+\nknJOuO9DOrRoGBwLaJyZFtYPHK//8pUfDaVlowyaN8rg0SnL6dOhGb+JswwCwF8uO5bZq7aFDTKH\nevSSgdzy8tyoj9VEdUs9X/V8eLfPr1+vDGChA7gBz89Yyfy1O6pkEmWm+Zi3tnL9/r3F5Uz1Jou9\nMWdt2BdKNHtLysnKSOONr9YB/gXoHrigP3u8DWLmrN7BrqJSCovKgl/koZvHTFqwMbgl5BXPzaS8\nwrFq/Nn88O9fhk18C7jjvwvo2KIhJ/Vow6A/fEB20wZ8dOuI4ONXPjeTVVv3MO2275AWMf/jrXnr\nOaZj5aqppeUV7C0uD+teTMS8NTuYunQzt4w6KtiVVlxWQYP0tBq9TjSlh0DLXcFdas3MePOmYZz3\n1xlURPwUb9s0K2xGbSzNsjJolpXBk1ccR25Oq2DrL7RbJ+Dhiwfwi1fmkZXhC7bcnr5yEMfntAwG\n//u+fwzLNlW/2uWJ3VuzIErwBLjjzN40yoz/P/hlgzszYdaaKudjDf4mKjB7F+CuN6J/QUVO9AJI\nTzOmhywl8b2/zQgeVxfYATbuLGLCrNXBDV4mzt9Aw4w0hh9VucDfiIemhS31PPjeD8NeY94a/98z\nEPwrKlzUwB6wp7iMd7/eyO6iMnYXlZFz+ztV9ix4Zvo3/Ojkyp3N8lZtY8yEOWGv89N/f8XkhZsS\n3mJy064imjfM4Ly/+v9GN5/aM1jnotLyWq3bFCmymzLgmU++oWe7Jozo1Xa/36M6Cu6yX7q3bUK6\nzxgzcv+yLc7o5/8iCPxUv210ryplzj+uE+cf14lPlhVw1fOzGHV0W0b3bV+lXMOM8MB8cW4nXskL\nn7TUODM9LFe9WVY6u7z89xuGd2N6fvRUzll3nkpxWQUdWzRkWI82/PTf4YHm3ZuH88MX84KBLtRt\no4/ij+8fmJUzJ8xaQ/tmlZk0K7fUbFDzrL9U7bt+dfZaXg3pFkp0Df+APSXx5xPc9uo89kT8Aonc\ns2Dh+l0UhHxZLo6S6TR5ob/VXVpeQUacgeVPl28hM93HxU99zikhX1oPf7As+Mszf1Mh495exK/P\n6E3nVv5fkXPX7CDdZ/TrWHWPhYDdRaU8+N7S4P1Zq7YFt90Mda83vnIw9jpWn7vslyYN0sm/76yo\nOdi1YWasGn82N30n9szak3q04bff7cMjlwyM+nhkq/tnI3sy5tSevP7jocFzWRm+YD/z3ef0CctH\nj7UMxNeZEx4gAAANGUlEQVS/P522zbLo3KoRPp9xTv+q//O2apzJyT3bVDn/9JWD+OnInvTrGH3S\nWGQGU21s3FVE68ZVUynry9Sl8fdsiAzs0bw9b33YhLRNu2L/KtoTscfwzn2lHH/vFL5ctY23563n\niudmBscgPg75lfPYR/nB40/ztzBx/gYuf/YLwD+P4Ht/ncE5j33K3DU72BqSaTQjf0vw/h8nL+Wf\nX1R28Y2ZMIe9JWW8v3BjcFbzwabgLknH5zOuHdY1Zi51o8zKH6Tv3jyczq0a8YvTjmJQl1Y8e1Uu\nPxnRHTPje8cewXcHHMHFuZ3wed06R3qtteKIbqEHLjiGJlHSO3u3b0qbJpUB1cy47qSunNmvfVjm\nT+AXxsW5nUn3GV/dfVpwUbibT+3J5SccGfN6zfzzD2bdeWqVWcITbhgSdr/PEbHXADrYIrtP6sIr\neVW7wgL+8M5i3pm/gTfnruP12WuZt2YHBbuLuejJz/lZgnUJfEGs2eYfCD/pgcqNZr731xkM+sMU\nZq3cxrw1O/jBszN5fGo+q7fuZd2OfVVeq8/Yydz4z9mc/ZdPqzz2TcGBX5tI3TKScgKbk0PV5RVG\n9WkX/JUxqEsrBnXxZ+cEGu4vXX8CQJUvjsjJQwGTxgzHAd3vnBQ816JRJk946/C8PnstPdtVzla8\namgOV5zQxfuCyuGhyUuDvyBicY7gTNmnrsxlT3EZ//ev2Yzs3Zah3VuHle1zRLOEZwc3zkzjV2f0\n5rdvLay+8CEicjyjT4dmwUlpr81eG5ZddFWMJTfiCR1gH//ukqhlLn7q8+AvrRdmrOKFGatq/D6v\nzl4b/EwPFLXcJeXUZmZgoFUe6JIZ2r01z1yVy6w7T+XHI7pz8lHRdw7z+axKNkeoub8dzesRSxgH\n3iOQbRRIv5s0ZjiNvS6l0Fb/I5cMCHt+4wbp/PO6E6IuVXxSj6pdQtGMObUnC8edwdUn5tCpZcOw\nx1bcd1bYF+SBdnYCA++xxJtt/O7XG2v9ukBYimqkDxbV7LX3lZTz5McrgvcPRveZWu6SksaM7MHx\nITnz1Xn26uP571drOaJ5ZQs9MDM2kRbWe7cMj7qBebzAXxnc/S33Pkc0I+83p1FUWk7Lxpn8LmSS\nWKICS0eEGndeX8Z6a/T/+ozePPDeEkb2rszWeP/nJ1Na7vjXF9/y7PRvSPNZWLbH8J5taJyZzk9H\n9uCcx/xdDJce35klG3dTWl7BwpBZonPuPo1j74k+0zbdZzx0Uf8q2TtHtWvKOwvqfuO2gv3IWqpO\ndeMJkW57dV7YNWY3bRCndN1Qy11S0i9G92J4z8T36e3apjG3ju5V6/VAerdvxlk1bIEG1uk5KmSR\nqYaZacGdqWqqS+tGtGycyWjvS+nqoV1YNO50rhzSJTg5bECn5qwafzYDO1du49goM53mDTO46Ts9\nmDN2NEAw0+P0vu14/PLjePLKQWHZIuMv6M8bPzmRlt5aOP07Nef+84+hZeNM/nfTMF64tnJNnYcv\nHsDXvz+d/PvO4vvHVp2lfPWJXbjnvL4xfx3F8s6Yk4LHoauYRjruyBbB+jTOTOPe7/eLWXZ/xVow\nLPLLq3XjAx/c1XIXqSdDu7dm4s9OqrJKZW29/TN/sPvbD46jpLwibGD5huHdmLlyG70TfK/xF/Tn\n9rN6Vxlr+POlA9ntpYyaGY9eOpAZ+Vs4b2DHYJmBnVtQFjJ7NqdN47DB6Ju+050NO4oYeGQL/vT+\nMpo3zODKoTn84IQulFZU0Os3/kXnTj4qO2wbSIBrh+UE+7j7HtGcG4Z3JT3NxwldK8ceXvzh4OCM\n2r/94DjOOqZDMN++V/um/OCELjz18Tesjlhu4IfDuvL8jJUALBp3On3GRt9nIJ6fjewRtixELG2a\nqltGJKXFy52uqcDkm/S0qksNj+rTrka51ZnpvqiDyKFBHPzbQEaeC9ShXbMGbNpVHDaTGeCXp1d2\nc101NCd47PMZDXyVaaw5rRsRuiDExJ+dRL+OzcMGMO86u0/wuGfbJizfXBhczgII/prK8DZ2D0xW\nOqJFVpXg/uMR3YPBPfDFeEbf9nRq2ZBnP11Z5RoDTjkqO5hamZWR2OzWg9FyV7eMiBwQgfTSRANe\nwJd3jWLmnacGZyv/6JRufHHHqdV+Ed7gzWTt2qYxlw3uHNyNDCDd5w915d7Mtb9eflyV50fOj1g1\n/myeuOI4fnNOH344rCsPXtA/6vueGJKxNLpPOx66MLzcqKOrzgFppQFVEUlWgeCcyHLEoQKDjYEh\n3fbNsmgfMtD93i3DyYqy/svFuZ25ONe/mN3954cH2MBrDvOyiVqHbDx/SW5nvndsxyozm6Ey82rs\nd/2/EE49ui0VDo6/dwoA03/1HTo0z+J+L23SzLgot3PYImd/uWwgC9fv4qInPw+eizfQXlcU3EWS\n3Ge3j6zS9XEo2N/NKgJzFLq2aRx2vnf7mo9RtG+eFQzEAb3aNaVX+6Y8cGH0Fnk0gS+FybeczMot\ne4JLFDx15SBWhExMCqwxtOSeM8jKSAv74rhwUPylr+uK1dfej7m5uS4vL/qmxyKS/N5dsIFfvT6f\nL+8aVeOuGfCvM7Rw/a46HZeoTuSWi7VVWFxGWXlFcGct5xx/m7aCWSu38dBF/WNOikuEmc12zsVO\nDwqUU3AXEfH7dPkWNuzcx0W5nasvXE8SDe7qlhER8ZwUZdG3ZFVtR52ZPW9mm80s6uLSZjbCzHaa\n2VzvNrbuqykiIjWRSMv978DjwD/ilJnunDunTmokIiL7rdqWu3PuEyD2dioiInLIqav8qaFmNs/M\n3jWzmKsdmdmNZpZnZnkFBTVbeEdERBJXF8H9K6CLc24A8Bjwv1gFnXNPO+dynXO52dk1WyRIREQS\nt9/B3Tm3yzlX6B1PAjLMLHWGnEVEktB+B3cza2/eVDQzG+y95tb9fV0REam9arNlzGwCMAJoY2Zr\ngd8CGQDOuSeBC4Efm1kZsA+41NXXzCgREQHqcYaqmRUA31ZbMLo2QGIbRaYOXfPhQdd8eNifa+7i\nnKt20LLegvv+MLO8RKbfphJd8+FB13x4OBjXfOgtJSciIvtNwV1EJAUla3B/ur4rUA90zYcHXfPh\n4YBfc1L2uYuISHzJ2nIXEZE4ki64m9kZZrbUzPLN7Pb6rk9dMbPOZjbVzBab2UIzu9k738rMPjCz\n5d5/W3rnzcz+4v0d5ptZ1R1/k4CZpZnZHDOb6N3vamYzvet92cwyvfMNvPv53uM59Vnv/WFmLczs\nNTNb4n3eQ1P5czazn3v/pr82swlmlpWKn3O05dFr87ma2dVe+eVmdnVt65NUwd3M0oC/AmcCfYDL\nzKxP/daqzpQBtzrnjgaGADd513Y78KFzrifwoXcf/H+Dnt7tRuCJg1/lOnEzsDjk/gPAI971bgeu\n885fB2x3zvUAHvHKJas/A+8553oDA/Bff0p+zmbWERgD5Drn+gFpwKWk5uf8d+CMiHM1+lzNrBX+\niaInAIOB3wa+EGrMOZc0N2AoMDnk/h3AHfVdrwN0rW8CpwFLgQ7euQ7AUu/4KeCykPLBcslyAzp5\n/+BHAhMBwz+xIz3y8wYmA0O943SvnNX3NdTimpsBKyPrnqqfM9ARWAO08j63icDpqfo5AznA17X9\nXIHLgKdCzoeVq8ktqVruVP5DCVjrnUsp3k/RY4GZQDvn3AYA779tvWKp8Ld4FPgVUOHdbw3scM6V\nefdDryl4vd7jO73yyaYbUAC84HVHPWtmjUnRz9k5tw74I7Aa2ID/c5tN6n/OATX9XOvs80624G5R\nzqVUuo+ZNQFeB25xzu2KVzTKuaT5W5jZOcBm59zs0NNRiroEHksm6cBxwBPOuWOBPVT+VI8mqa/b\n61I4D+gKHAE0xt8lESnVPufqxLrOOrv+ZAvua4HQbck7AevrqS51zswy8Af2l5xz//VObzKzDt7j\nHYDN3vlk/1sMA841s1XAf/B3zTwKtDCzwIJ2odcUvF7v8eYk5w5ha4G1zrmZ3v3X8Af7VP2cRwEr\nnXMFzrlS4L/AiaT+5xxQ08+1zj7vZAvuXwI9vZH2TPwDM2/Vc53qhJkZ8Byw2Dn3cMhDbwGBEfOr\n8ffFB85f5Y26DwF2Bn7+JQPn3B3OuU7OuRz8n+NHzrkfAFPxrzQKVa838He40CufdC0659xGYI2Z\n9fJOnQosIkU/Z/zdMUPMrJH3bzxwvSn9OYeo6ec6GRhtZi29Xz2jvXM1V98DELUYsDgLWAasAO6q\n7/rU4XWdhP/n13xgrnc7C39/44fAcu+/rbzyhj9zaAWwAH82Qr1fRy2vfQQw0TvuBswC8oFXgQbe\n+Szvfr73eLf6rvd+XO9AIM/7rP8HtEzlzxn4PbAE+Br4J9AgFT9nYAL+cYVS/C3w62rzuQI/9K4/\nH7i2tvXRDFURkRSUbN0yIiKSAAV3EZEUpOAuIpKCFNxFRFKQgruISApScBcRSUEK7iIiKUjBXUQk\nBf0/wea7RWih8lQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8408ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating at different \"temperatures\"\n",
    "\n",
    "In the `evaluate` function above, every time a prediction is made the outputs are divided by the \"temperature\" argument passed. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. As we turn the temperature towards zero we are choosing only the most likely outputs.\n",
    "\n",
    "We can see the effects of this by adjusting the `temperature` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thrice as that is and neit be to-\n",
      "it.\n",
      "\n",
      "KING HERIOLA:\n",
      "Of the king, take are my ingris;\n",
      "And there she speek come your some for me against sir, I have courtamble well him as true the nood for this all see?\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower temperatures are less varied, choosing only the more probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grace the come the can the can the sir, the man the come the contersed the so the conter the was the come the maning the grace the put the man the man the see the man the can the see the shall the c\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher temperatures more varied, choosing less probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thwatainamich\n",
      "From forsly\n",
      "Lolord:\n",
      "'t must not tate I tarm\n",
      "Lorss'd of am\n",
      "kince you; Alx he nour now\n",
      "at qullo oppor,\n",
      "And gron witty, noireen; herank Honers, no thisal amabo! Sugly laizan Isled\n",
      "drea-cueth \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "* Train with your own dataset, e.g.\n",
    "    * Text from another author\n",
    "    * Blog posts\n",
    "    * Code\n",
    "* Increase number of layers and network size to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
