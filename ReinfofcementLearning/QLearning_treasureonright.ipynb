{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习 Q learning算法\n",
    "这一次我们会用 tabular Q-learning 的方法实现一个小例子, 例子的环境是一个一维世界, 在世界的右边有宝藏, 探索者只要得到宝藏尝到了甜头, 然后以后就记住了得到宝藏的方法, 这就是他用强化学习所学习到的行为.\n",
    "\n",
    "-o---T\n",
    "\n",
    "#T 就是宝藏的位置, o 是探索者的位置\n",
    "Q-learning 是一种记录行为值 (Q value) 的方法, 每种在一定状态的行为都会有一个值 Q(s, a), 就是说 行为 a 在 s 状态的值是 Q(s, a). s 在上面的探索者游戏中, 就是 o 所在的地点了. 而每一个地点探索者都能做出两个行为 left/right, 这就是探索者的所有可行的 a 啦.\n",
    "\n",
    "如果在某个地点 s1, 探索者计算了他能有的两个行为, a1/a2=left/right, 计算结果是 Q(s1, a1) > Q(s1, a2), 那么探索者就会选择 left 这个行为. 这就是 Q learning 的行为选择简单规则."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "N_STATES = 7     #一维世界的长度\n",
    "ACTIONS = ['left','right'] #可采取的行动\n",
    "EPSILON = 0.9   #贪婪度\n",
    "ALPHA = 0.1    #learning rate\n",
    "GAMMA = 0.9     #奖励递减值\n",
    "MAX_EPISODES = 13 #最大回合数\n",
    "FRESH_TIME = 0.3  #移动间隔时间\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q表\n",
    "对于 tabular Q learning, 我们必须将所有的 Q values (行为值) 放在 q_table 中, 更新 q_table 也是在更新他的行为准则. q_table 的 index 是所有对应的 state (探索者位置), columns 是对应的 action (探索者行为)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_q_table(n_states,actions):\n",
    "    table = pd.DataFrame(\n",
    "          np.zeros((n_states,len(actions))),#q_table全0初始化\n",
    "          columns = actions,  # columns对应行为名称\n",
    "    )\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 定义动作\n",
    " 接着定义探索者是如何挑选行为的. 这是我们引入 epsilon greedy 的概念. 因为在初始阶段, 随机的探索环境, 往往比固定的行为模式要好, 所以这也是累积经验的阶段, 我们希望探索者不会那么贪婪(greedy). 所以 EPSILON 就是用来控制贪婪程度的值. EPSILON 可以随着探索时间不断提升(越来越贪婪), 不过在这个例子中, 我们就固定成 EPSILON = 0.9, 90% 的时间是选择最优策略, 10% 的时间来探索."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state,q_table):\n",
    "    state_actions = q_table.iloc[state,:] # 选出这个 state 的所有 action 值\n",
    "    if(np.random.uniform()>EPSILON)or(state_actions.all()==0): # 非贪婪 or 或者这个 state 还没有探索过\n",
    "        action_name =  np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        action_name = state_actions.argmax()#贪婪模式\n",
    "    return action_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境反馈\n",
    "做出行为后, 环境也要给我们的行为一个反馈, 反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R). 这里定义的规则就是, 只有当 o 移动到了 T, 探索者才会得到唯一的一个奖励, 奖励值 R=1, 其他情况都没有奖励."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_env_feedback(S,A):\n",
    "    if A == 'right':\n",
    "        if S==N_STATES-2: #terminate\n",
    "            S_ = 'terminal'\n",
    "            R=1\n",
    "        else:\n",
    "            S_=S+1\n",
    "            R=0\n",
    "    else:\n",
    "        R=0\n",
    "        if S==0:\n",
    "            S_=S\n",
    "        else:\n",
    "            S_=S-1\n",
    "    return S_,R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#环境更新\n",
    "def update_env(S,episode,step_counter):\n",
    "    env_list = ['-']*(N_STATES-1)+['T']\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s:total_steps=%s'%(episode+1,step_counter)\n",
    "        print('\\r{}'.format(interaction),end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r         ',end='')\n",
    "    else:\n",
    "        env_list[S]='o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction),end='')\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习主循环\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl():\n",
    "    q_table = build_q_table(N_STATES,ACTIONS) #初始化q table\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False  #是否回合结束\n",
    "        update_env(S,episode,step_counter) #更新环境\n",
    "        while not is_terminated:\n",
    "            A = choose_action(S,q_table) #选择行为\n",
    "            S_,R = get_env_feedback(S,A)\n",
    "            q_predict = q_table.loc[S,A]\n",
    "            if S_!='terminal':\n",
    "                q_target = R+GAMMA*q_table.iloc[S_,:].max()  #  实际的(状态-行为)值 (回合没结束)\n",
    "            \n",
    "            else:\n",
    "                q_target = R\n",
    "                is_terminated = True\n",
    "            q_table.loc[S,A]+=ALPHA*(q_target-q_predict) #q_table跟新\n",
    "            S=S_ #探索者移动到下一个state\n",
    "            update_env(S,episode,step_counter+1)\n",
    "            \n",
    "            step_counter +=1\n",
    "        return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         :total_steps=45\n",
      "Q-table:\n",
      "\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.1\n",
      "6   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
